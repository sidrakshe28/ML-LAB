{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"face_recog.ipynb","provenance":[]},"kernelspec":{"name":"python361064bitvisualstudiocodeconda2733b479a2d14b7ca82f722b5f735a0a","display_name":"Python 3.6.10 64-bit ('Visual Studio Code': conda)"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mK5RsAcCrXf5"},"source":["import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cy4Lk9LvreLI"},"source":["files = os.listdir(\"C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\\\Lab\\\\p_identify\\\\train\")\n","class_image = []\n","for x in files:\n","  output = x.split('.')[0]\n","  if(output == 'aditya'):\n","    class_image.append('aditya')\n","  \n","  elif(output == 'ameya'):\n","    class_image.append('ameya')\n","  elif(output == 'sagarika'):\n","    class_image.append('sagarika')\n","  elif(output == 'ashray'):\n","    class_image.append('ashray')\n","  elif(output == 'abhishek'):\n","    class_image.append('abhishek')\n","  elif(output == 'raghav'):\n","    class_image.append('raghav')\n","  elif(output == 'rayan'):\n","    class_image.append('rayan')\n","  elif(output == 'karan'):\n","    class_image.append('karan')\n","  elif(output == 'sayali'):\n","    class_image.append('sayali')\n","  elif(output == 'shubham'):\n","    class_image.append('subham')\n","  elif(output == 'mayank'):\n","    class_image.append('mayank')\n","  elif(output == 'pratik'):\n","    class_image.append('pratik')\n","  elif(output == 'rutuja'):\n","    class_image.append('rutuja')\n","  elif(output == 'sharvari'):\n","    class_image.append('sharvari')\n","  elif(output == 'shivani'):\n","    class_image.append('shivani')\n","  elif(output == 'shruti'):\n","    class_image.append('shruti')\n","  elif(output == 'unzela'):\n","    class_image.append('unzela')\n","  elif(output == 'siddhika'):\n","    class_image.append('siddhika')\n","  elif(output == 'vishal'):\n","    class_image.append('vishal')\n","  elif(output == 'shraddha'):\n","    class_image.append('shraddha')\n","df = pd.DataFrame({'filename':files, 'class':class_image})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xj7cPyjUnkve","outputId":"4de17ac6-c458-45a1-b4e4-ac2388fd72a1"},"source":["classes = os.listdir(\"C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\Lab\\\\p_identify\\\\photos\")\n","print(classes)\n","\n","os.makedirs(\"Faces\",exist_ok=True)\n","face_db='C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\\\Lab\\\\p_identify\\\\Faces'\n","\n","data_dir = 'C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\Lab\\\\p_identify\\\\photos'\n","\n","import dlib\n","import cv2 \n","from PIL import Image\n","import numpy as np\n","import glob\n","\n","face_detector = dlib.get_frontal_face_detector()\n","\n","for c in classes:\n","  os.makedirs(face_db+f\"/{c}\",exist_ok=True)\n","  i=0\n","  for f in glob.glob(data_dir+f\"/{c}/*\"):\n","\n","    img = Image.open(f).convert('L')\n","    detected_face = face_detector(np.array(img),1)\n","    if len(detected_face)==1:\n","      crop_area = (detected_face[0].left(), detected_face[0].top(), detected_face[0].right(), detected_face[0].bottom())\n","      cropped_image = img.crop(crop_area)\n","      \n","      name = c+str(i)\n","      cropped_image.save(face_db+f\"/{c}/{name}.jpg\")\n","      i=i+1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Aditya', 'Ameya', 'Anmol', 'Ashray', 'Dj', 'Habil', 'Jeevan', 'Karan', 'Kartik', 'Manas', 'Mayank', 'Pratik', 'Rutuja', 'Sharvari', 'Shivani', 'Shruti Agarwal', 'Shubham', 'Siddhika', 'Vishal', 'Yashi']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_ahzNf73nkve"},"source":["dataset = pd.DataFrame()\n","for c in classes:\n","  for f in glob.glob(face_db+f\"/{c}/*\"):\n","    \n","    dataset = dataset.append({\"filename\":f,\"class\":c},ignore_index=True)\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSAnuQJCrgg8"},"source":["test_ratio = 0.4\n","test = dataset.sample(frac=test_ratio,random_state=21)\n","train = dataset.drop(test.index, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICJyg75zsKIa"},"source":["train = train.reset_index(drop=True)\n","test = test.reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njnvB9vpsTMF"},"source":["# train_gen = ImageDataGenerator(\n","#     rotation_range=15,\n","#     rescale=1./255,\n","#     shear_range=0.1,\n","#     zoom_range=0.2,\n","#     horizontal_flip=True,\n","#     width_shift_range=0.1,\n","#     height_shift_range=0.1\n","# )\n","\n","# validation_gen = ImageDataGenerator(rescale=1./255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"daO2t_BtsaK4","colab":{"base_uri":"https://localhost:8080/","height":35},"tags":[],"outputId":"3fe8bb7c-41fb-4f30-93d3-29310565023d"},"source":["#    train_generator = train_gen.flow_from_dataframe(\n","#        train, \n","#         \"C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\\\Lab\\\\p_identify\\\\Faces\", \n","#         x_col='filename',\n","#         y_col='class',\n","#         target_size=(128,128),\n","#         class_mode='categorical',\n","#         batch_size=15\n","#     )\n","train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=15,\n","    rescale=1./255,\n","    zoom_range=0.2,\n","    )\n","train_gen = train_data_gen.flow_from_dataframe(dataframe=train,xcol='filename',ycol='class',target_size=(224,224),batch_size=8,class_mode='categorical')\n","\n","valid_data_gen =  tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","valid_gen = valid_data_gen.flow_from_dataframe(dataframe=test,xcol='filename',ycol='class',target_size=(224,224),batch_size=8,class_mode='categorical',random=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 96 validated image filenames belonging to 19 classes.\n","Found 64 validated image filenames belonging to 18 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iI_i-JKCvJsu","colab":{"base_uri":"https://localhost:8080/","height":35},"tags":[],"outputId":"b73aba87-febc-413b-e503-11787b138ade"},"source":["# validation_datagen = ImageDataGenerator(rescale=1./255)\n","# validation_generator = validation_datagen.flow_from_dataframe(\n","#     test, \n","#     \"C:\\\\Users\\\\karan\\\\Desktop\\\\4th year\\\\ML\\\\Lab\\\\p_identify\\\\train\", \n","#     x_col='filename',\n","#     y_col='class',\n","#     target_size=(128,128),\n","#     class_mode='categorical',\n","#     batch_size=15\n","# )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 60 validated image filenames belonging to 20 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JGHSu6v3nkvf","outputId":"e8166c8d-2812-4186-ca81-9e7f85c494ce"},"source":["from tensorflow.keras.layers import ZeroPadding2D, MaxPooling2D, Convolution2D, Dense, Activation, Flatten, Dropout\n","\n","from tensorflow import keras\n","\n","from tensorflow.keras import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","from tensorflow.keras.layers import Dense,Flatten,Conv2D,MaxPool2D,GlobalAveragePooling2D\n","\n","vgg16_model =VGG16(weights = 'imagenet', include_top = False)\n","x = vgg16_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(20, activation = 'softmax')(x)\n","model = Model(inputs = vgg16_model.input, outputs = predictions)\n","\n","for layer in vgg16_model.layers:\n","  layer.trainable = False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 237s 4us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JOfOMpZQnkvf"},"source":["model.compile(loss='categorical_crossentropy',\n","                     optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oc0wlBa7nkvf","outputId":"a0addca6-e951-4f26-b453-b7ebca6d4a14"},"source":["model.fit(train_gen,epochs=10,validation_data=valid_gen)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","Train for 12 steps, validate for 8 steps\n","Epoch 1/10\n"," 1/12 [=>............................] - ETA: 1:57"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":" logits and labels must be broadcastable: logits_size=[8,20] labels_size=[8,19]\n\t [[node loss/dense_1_loss/softmax_cross_entropy_with_logits (defined at <ipython-input-39-1200dd3cedca>:1) ]] [Op:__inference_distributed_function_1484]\n\nFunction call stack:\ndistributed_function\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[1;32m<ipython-input-39-1200dd3cedca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n","\u001b[1;32m~\\Anaconda3\\envs\\Visual Studio Code\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n","\u001b[1;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[8,20] labels_size=[8,19]\n\t [[node loss/dense_1_loss/softmax_cross_entropy_with_logits (defined at <ipython-input-39-1200dd3cedca>:1) ]] [Op:__inference_distributed_function_1484]\n\nFunction call stack:\ndistributed_function\n"]}]},{"cell_type":"code","metadata":{"id":"jLOYtV6Zrwye"},"source":["# model = Sequential()\n","\n","# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n","# model.add(MaxPooling2D(pool_size=(2, 2)))\n","# model.add(Dropout(0.25))\n","\n","# model.add(Conv2D(64, (3, 3), activation='relu'))\n","# model.add(MaxPooling2D(pool_size=(2, 2)))\n","# model.add(Dropout(0.25))\n","\n","# model.add(Conv2D(128, (3, 3), activation='relu'))\n","# model.add(MaxPooling2D(pool_size=(2, 2)))\n","# model.add(Dropout(0.25))\n","\n","# model.add(Flatten())\n","# model.add(Dense(512, activation='relu'))\n","# model.add(Dropout(0.5))\n","# model.add(Dense(20, activation='softmax')) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"-jg5gDe_nkvf","outputId":"b90564d3-5152-4018-a3dd-47e6aced8fde"},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_3 (Conv2D)            (None, 126, 126, 32)      896       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 63, 63, 32)        0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 63, 63, 32)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 61, 61, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 30, 30, 64)        0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 30, 30, 64)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 28, 28, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 25088)             0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               12845568  \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 20)                10260     \n","=================================================================\n","Total params: 12,949,076\n","Trainable params: 12,949,076\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Enf0n8jCr8vm","colab":{"base_uri":"https://localhost:8080/","height":1000},"tags":[],"outputId":"f885f79f-c13a-482a-c44f-99bc584511d2"},"source":["history = model.fit(\n","    train_generator, \n","    epochs=20,\n","    validation_data=validation_generator\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","Train for 7 steps, validate for 4 steps\n","Epoch 1/20\n","7/7 [==============================] - 9s 1s/step - loss: 3.8737 - accuracy: 0.0800 - val_loss: 2.9980 - val_accuracy: 0.0500\n","Epoch 2/20\n","7/7 [==============================] - 8s 1s/step - loss: 3.0018 - accuracy: 0.0500 - val_loss: 2.9989 - val_accuracy: 0.0167\n","Epoch 3/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.9919 - accuracy: 0.0600 - val_loss: 3.0024 - val_accuracy: 0.0167\n","Epoch 4/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.9773 - accuracy: 0.0500 - val_loss: 3.0183 - val_accuracy: 0.0167\n","Epoch 5/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.9641 - accuracy: 0.0700 - val_loss: 3.0432 - val_accuracy: 0.0333\n","Epoch 6/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.9578 - accuracy: 0.0600 - val_loss: 3.0220 - val_accuracy: 0.0500\n","Epoch 7/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.9444 - accuracy: 0.1300 - val_loss: 3.0155 - val_accuracy: 0.0500\n","Epoch 8/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.8859 - accuracy: 0.0800 - val_loss: 3.0042 - val_accuracy: 0.0667\n","Epoch 9/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.8394 - accuracy: 0.1700 - val_loss: 2.9777 - val_accuracy: 0.0667\n","Epoch 10/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.6922 - accuracy: 0.1900 - val_loss: 2.8907 - val_accuracy: 0.0833\n","Epoch 11/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.5500 - accuracy: 0.2700 - val_loss: 2.7973 - val_accuracy: 0.1833\n","Epoch 12/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.5024 - accuracy: 0.2300 - val_loss: 2.7326 - val_accuracy: 0.2000\n","Epoch 13/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.4257 - accuracy: 0.2400 - val_loss: 2.6708 - val_accuracy: 0.2333\n","Epoch 14/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.2544 - accuracy: 0.3300 - val_loss: 2.6239 - val_accuracy: 0.2667\n","Epoch 15/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.0420 - accuracy: 0.3700 - val_loss: 2.6481 - val_accuracy: 0.2000\n","Epoch 16/20\n","7/7 [==============================] - 8s 1s/step - loss: 2.0073 - accuracy: 0.4500 - val_loss: 2.3652 - val_accuracy: 0.3000\n","Epoch 17/20\n","7/7 [==============================] - 8s 1s/step - loss: 1.7892 - accuracy: 0.4300 - val_loss: 2.4101 - val_accuracy: 0.3333\n","Epoch 18/20\n","7/7 [==============================] - 8s 1s/step - loss: 1.8982 - accuracy: 0.4000 - val_loss: 2.3758 - val_accuracy: 0.3000\n","Epoch 19/20\n","7/7 [==============================] - 8s 1s/step - loss: 1.6361 - accuracy: 0.5300 - val_loss: 2.2874 - val_accuracy: 0.3167\n","Epoch 20/20\n","7/7 [==============================] - 8s 1s/step - loss: 1.5664 - accuracy: 0.5300 - val_loss: 2.3024 - val_accuracy: 0.3667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bpg7jbeVwXZH"},"source":["#model.save(\"model-sample-20-2.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pS_i6xxVnkvf","outputId":"132727f0-7030-4e0c-a140-54bc4e64888b"},"source":["model.evaluate(train_generator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","7/7 [==============================] - 4s 547ms/step - loss: 1.2757 - accuracy: 0.6600\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[1.2756736278533936, 0.66]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"xGT12Czankvf","outputId":"9b080507-9e06-4d02-d151-026afd456192"},"source":["score = model.evaluate(validation_generator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:sample_weight modes were coerced from\n","  ...\n","    to  \n","  ['...']\n","4/4 [==============================] - 2s 412ms/step - loss: 2.3024 - accuracy: 0.3667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qloms7w_nkvf","outputId":"fab46996-e97d-4cba-e002-fa98477ad015"},"source":["print('Test loss:', score[0])\n","print('Test accuracy:', score[1]*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss: 2.3023688197135925\n","Test accuracy: 36.666667461395264\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V0MRAN_nnkvf"},"source":[""],"execution_count":null,"outputs":[]}]}